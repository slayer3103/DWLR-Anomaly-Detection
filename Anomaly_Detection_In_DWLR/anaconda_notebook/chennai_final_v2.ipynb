{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8157b42-7e61-4fb1-8b9d-7edb647d3004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Depth...\n",
      "Epoch 1/50\n",
      "730/730 [==============================] - 14s 16ms/step - loss: 0.0980 - val_loss: 0.0596\n",
      "Epoch 2/50\n",
      "730/730 [==============================] - 11s 16ms/step - loss: 0.0458 - val_loss: 0.0401\n",
      "Epoch 3/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0401 - val_loss: 0.0376\n",
      "Epoch 4/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0364 - val_loss: 0.0354\n",
      "Epoch 5/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0332 - val_loss: 0.0300\n",
      "Epoch 6/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0292 - val_loss: 0.0394\n",
      "Epoch 7/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0262 - val_loss: 0.0275\n",
      "Epoch 8/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0254 - val_loss: 0.0239\n",
      "Epoch 9/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0235 - val_loss: 0.0221\n",
      "Epoch 10/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 11/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0205 - val_loss: 0.0218\n",
      "Epoch 12/50\n",
      "730/730 [==============================] - 11s 16ms/step - loss: 0.0195 - val_loss: 0.0204\n",
      "Epoch 13/50\n",
      "730/730 [==============================] - 11s 16ms/step - loss: 0.0188 - val_loss: 0.0194\n",
      "Epoch 14/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0181 - val_loss: 0.0171\n",
      "Epoch 15/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0169 - val_loss: 0.0224\n",
      "Epoch 16/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0172 - val_loss: 0.0199\n",
      "Epoch 17/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0161 - val_loss: 0.0154\n",
      "Epoch 18/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0164 - val_loss: 0.0169\n",
      "Epoch 19/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0153 - val_loss: 0.0150\n",
      "Epoch 20/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 21/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0150 - val_loss: 0.0174\n",
      "Epoch 22/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0143 - val_loss: 0.0151\n",
      "Epoch 23/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0149 - val_loss: 0.0157\n",
      "Epoch 24/50\n",
      "730/730 [==============================] - 11s 16ms/step - loss: 0.0143 - val_loss: 0.0129\n",
      "Epoch 25/50\n",
      "730/730 [==============================] - 11s 15ms/step - loss: 0.0137 - val_loss: 0.0174\n",
      "Epoch 26/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0136 - val_loss: 0.0128\n",
      "Epoch 27/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0134 - val_loss: 0.0125\n",
      "Epoch 28/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0132 - val_loss: 0.0117\n",
      "Epoch 29/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0130 - val_loss: 0.0138\n",
      "Epoch 30/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 31/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0132 - val_loss: 0.0159\n",
      "Epoch 32/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 33/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0125 - val_loss: 0.0140\n",
      "Model saved to C:/Users/saura/Downloads/trained_model_v2\\depth_model_chennai.h5\n",
      "Training model for Temperature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\anomaly_detection\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "730/730 [==============================] - 13s 14ms/step - loss: 0.0975 - val_loss: 0.0335\n",
      "Epoch 2/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0330 - val_loss: 0.0335\n",
      "Epoch 3/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0304 - val_loss: 0.0313\n",
      "Epoch 4/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0288 - val_loss: 0.0268\n",
      "Epoch 5/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0279 - val_loss: 0.0285\n",
      "Epoch 6/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0272 - val_loss: 0.0264\n",
      "Epoch 7/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0269 - val_loss: 0.0316\n",
      "Epoch 8/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0275 - val_loss: 0.0226\n",
      "Epoch 9/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0266 - val_loss: 0.0216\n",
      "Epoch 10/50\n",
      "730/730 [==============================] - 11s 14ms/step - loss: 0.0250 - val_loss: 0.0221\n",
      "Epoch 11/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0248 - val_loss: 0.0235\n",
      "Epoch 12/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0237 - val_loss: 0.0207\n",
      "Epoch 13/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0264 - val_loss: 0.0233\n",
      "Epoch 14/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0248 - val_loss: 0.0224\n",
      "Epoch 15/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0230 - val_loss: 0.0192\n",
      "Epoch 16/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0213 - val_loss: 0.0199\n",
      "Epoch 17/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0203 - val_loss: 0.0181\n",
      "Epoch 18/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0199 - val_loss: 0.0180\n",
      "Epoch 19/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0185 - val_loss: 0.0187\n",
      "Epoch 20/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0177 - val_loss: 0.0154\n",
      "Epoch 21/50\n",
      "730/730 [==============================] - 10s 13ms/step - loss: 0.0177 - val_loss: 0.0158\n",
      "Epoch 22/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 23/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0167 - val_loss: 0.0151\n",
      "Epoch 24/50\n",
      "730/730 [==============================] - 11s 14ms/step - loss: 0.0171 - val_loss: 0.0195\n",
      "Epoch 25/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0169 - val_loss: 0.0146\n",
      "Epoch 26/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0160 - val_loss: 0.0133\n",
      "Epoch 27/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0159 - val_loss: 0.0139\n",
      "Epoch 28/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0155 - val_loss: 0.0136\n",
      "Epoch 29/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0154 - val_loss: 0.0133\n",
      "Epoch 30/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0149 - val_loss: 0.0137\n",
      "Epoch 31/50\n",
      "730/730 [==============================] - 10s 14ms/step - loss: 0.0154 - val_loss: 0.0159\n",
      "Model saved to C:/Users/saura/Downloads/trained_model_v2\\temperature_model_chennai.h5\n",
      "Training model for Battery Level...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\anomaly_detection\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "730/730 [==============================] - 18s 20ms/step - loss: 0.0426 - val_loss: 0.0139\n",
      "Epoch 2/50\n",
      "730/730 [==============================] - 15s 21ms/step - loss: 0.0183 - val_loss: 0.0155\n",
      "Epoch 3/50\n",
      "730/730 [==============================] - 16s 21ms/step - loss: 0.0130 - val_loss: 0.0066\n",
      "Epoch 4/50\n",
      "730/730 [==============================] - 17s 23ms/step - loss: 0.0090 - val_loss: 0.0069\n",
      "Epoch 5/50\n",
      "730/730 [==============================] - 19s 25ms/step - loss: 0.0089 - val_loss: 0.0170\n",
      "Epoch 6/50\n",
      "730/730 [==============================] - 18s 25ms/step - loss: 0.0079 - val_loss: 0.0071\n",
      "Epoch 7/50\n",
      "730/730 [==============================] - 18s 25ms/step - loss: 0.0072 - val_loss: 0.0046\n",
      "Epoch 8/50\n",
      "730/730 [==============================] - 19s 25ms/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 9/50\n",
      "730/730 [==============================] - 18s 25ms/step - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 10/50\n",
      "730/730 [==============================] - 18s 25ms/step - loss: 0.0059 - val_loss: 0.0069\n",
      "Epoch 11/50\n",
      "730/730 [==============================] - 18s 24ms/step - loss: 0.0066 - val_loss: 0.0109\n",
      "Epoch 12/50\n",
      "730/730 [==============================] - 18s 25ms/step - loss: 0.0063 - val_loss: 0.0050\n",
      "Model saved to C:/Users/saura/Downloads/trained_model_v2\\battery_mode_chennail.h5\n",
      "Scalers saved to C:/Users/saura/Downloads/trained_model_v2\\scalers_chennai.pkl\n",
      "Training complete. Models and scalers are saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\anomaly_detection\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.signal import detrend\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# ==============================\n",
    "# Step 1: Load and Preprocess Data\n",
    "# ==============================\n",
    "def load_and_preprocess_data(file_path, target_column, sequence_length, scaler_type='StandardScaler', detrend_data=False):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Detrend the data if required (e.g., for Battery Level)\n",
    "    if detrend_data:\n",
    "        data[target_column] = detrend(data[target_column])\n",
    "\n",
    "    # Normalize the target column\n",
    "    if scaler_type == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    data[target_column] = scaler.fit_transform(data[[target_column]])\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[target_column].iloc[i:i + sequence_length].values\n",
    "        sequences.append(seq)\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    sequences = np.expand_dims(sequences, axis=2)  # Add feature dimension\n",
    "    return sequences, scaler\n",
    "\n",
    "# ==============================\n",
    "# Step 2: Define LSTM Autoencoder\n",
    "# ==============================\n",
    "def create_lstm_autoencoder(sequence_length, input_dim, latent_dim=32):\n",
    "    # Encoder\n",
    "    input_layer = Input(shape=(sequence_length, input_dim))\n",
    "    encoded = LSTM(latent_dim, activation='relu', return_sequences=False)(input_layer)\n",
    "    encoded = Dense(latent_dim // 2, activation='relu')(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(sequence_length)(encoded)\n",
    "    decoded = LSTM(latent_dim, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mae')\n",
    "    return autoencoder\n",
    "\n",
    "# ==============================\n",
    "# Step 3: Train and Save Models\n",
    "# ==============================\n",
    "def train_and_save_model(sequences, model_path, sequence_length, input_dim, latent_dim=32, epochs=50, batch_size=32):\n",
    "    # Define LSTM Autoencoder\n",
    "    model = create_lstm_autoencoder(sequence_length, input_dim, latent_dim)\n",
    "\n",
    "    # Train model with early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(sequences, sequences, epochs=epochs, batch_size=batch_size, validation_split=0.2, shuffle=True, callbacks=[early_stopping])\n",
    "\n",
    "    # Save model\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ==============================\n",
    "# Step 4: Main Script\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    file_path = \"C:/Users/saura/OneDrive/Desktop/data_streaming/data_generators/chennai/dwlrdata_chennai_anomalies_final(1).csv\"\n",
    "    sequence_length = 30         # Number of timesteps per sequence\n",
    "    epochs = 50                  # Training epochs\n",
    "    batch_size = 32              # Batch size\n",
    "    output_dir = \"C:/Users/saura/Downloads/trained_model_v2\"  # Directory to save models\n",
    "\n",
    "    # Create output directory if not exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Depth\n",
    "    print(\"Training model for Depth...\")\n",
    "    depth_sequences, depth_scaler = load_and_preprocess_data(\n",
    "        file_path, target_column=\"Depth\", sequence_length=sequence_length, scaler_type='StandardScaler'\n",
    "    )\n",
    "    train_and_save_model(depth_sequences, os.path.join(output_dir, \"depth_model_chennai.h5\"), sequence_length, input_dim=1, latent_dim=32, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Temperature\n",
    "    print(\"Training model for Temperature...\")\n",
    "    temperature_sequences, temperature_scaler = load_and_preprocess_data(\n",
    "        file_path, target_column=\"Temperature\", sequence_length=sequence_length, scaler_type='StandardScaler'\n",
    "    )\n",
    "    train_and_save_model(temperature_sequences, os.path.join(output_dir, \"temperature_model_chennai.h5\"), sequence_length, input_dim=1, latent_dim=32, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Battery Level\n",
    "    print(\"Training model for Battery Level...\")\n",
    "    battery_sequences, battery_scaler = load_and_preprocess_data(\n",
    "        file_path, target_column=\"Battery Level\", sequence_length=sequence_length, scaler_type='MinMaxScaler', detrend_data=True\n",
    "    )\n",
    "    train_and_save_model(battery_sequences, os.path.join(output_dir, \"battery_mode_chennail.h5\"), sequence_length, input_dim=1, latent_dim=64, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Save scalers for future use\n",
    "    scalers = {\n",
    "        \"Depth\": depth_scaler,\n",
    "        \"Temperature\": temperature_scaler,\n",
    "        \"Battery Level\": battery_scaler\n",
    "    }\n",
    "    scaler_path = os.path.join(output_dir, \"scalers_chennai.pkl\")\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scalers, f)\n",
    "    print(f\"Scalers saved to {scaler_path}\")\n",
    "\n",
    "    print(\"Training complete. Models and scalers are saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7747a0-e692-40cd-8fe5-c0f5938d940c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
